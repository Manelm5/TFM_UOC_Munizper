{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <h1 id=\"Librer%C3%ADas-y-variables-de-entorno\">Inicializacion del Entorno<a class=\"anchor-link\" href=\"#Librer%C3%ADas-y-variables-de-entorno\">¶</a></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vit_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UOC\n",
    "# Máster de Ciencia de Datos \n",
    "# TFM - Refinamientos de modelos de IA para la identificación de cáncer de piel\n",
    "# Manel Muñiz Peralvarez\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import asarray \n",
    "from datasets import load_dataset \n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from vit_keras import vit\n",
    "from tensorflow.keras.applications import EfficientNetB0, Xception, ResNet50, ResNet152, VGG16, DenseNet201, MobileNetV2, InceptionResNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "import warnings\n",
    "from PIL import Image\n",
    "import csv\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# GPU CUDA\n",
    "# Directorio con las librerías CUDA\n",
    "#os.add_dll_directory('C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v11.2\\\\bin')\n",
    "# Obtención de información de la GPU dispponible\n",
    "from tensorflow.python.client import device_lib\n",
    "def print_info():\n",
    "    print('  Versión de TensorFlow: {}'.format(tf.__version__))\n",
    "    print('  GPU: {}'.format([x.physical_device_desc for x in device_lib.list_local_devices() if x.device_type=='GPU']))\n",
    "    print('  Versión Cuda  -> {}'.format(tf.sysconfig.get_build_info()['cuda_version']))\n",
    "    print('  Versión Cudnn -> {}\\n'.format(tf.sysconfig.get_build_info()['cudnn_version']))\n",
    "print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_or = '/kaggle/input/datostfb/Datos/'\n",
    "\n",
    "# De todas las configuraciones realizadas en el trabajo anterior, usamos CF2 la cuál es la de salida binaria benigno/maligno\n",
    "CF = 'CF2'\n",
    "\n",
    "path = 'D:/TFM/Codigo/Datos/CF2/'\n",
    "path_img = 'D:/TFM/Codigo/Datos/CF2/imagenes/'\n",
    "output_path = 'D:/TFM/Codigo/Datos/CF2/salidas/'\n",
    "\n",
    "# Dimensiones de las imágenes para redes preentrenadas\n",
    "WIDTH_224 = 224\n",
    "HEIGHT_224 = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLAGS \n",
    "CF1_flag = False\n",
    "CF2_flag = True\n",
    "CF3_flag = False\n",
    "CF4_flag = False\n",
    "\n",
    "# Aquí seleccionaremos como \"True\" los modelos que queramos entrenar. Por defecto se entrenan todos\n",
    "train_model_EfficientNet_flag = True\n",
    "test_model_EfficientNet_flag = True\n",
    "\n",
    "train_model_xception_flag = True\n",
    "test_model_xception_flag = True\n",
    "\n",
    "train_model_resnet_flag = True\n",
    "test_model_resnet_flag = True\n",
    "\n",
    "train_model_VGG16_flag = True\n",
    "test_model_VGG16_flag = True\n",
    "\n",
    "train_model_densenet_flag = True\n",
    "test_model_densenet_flag = True\n",
    "\n",
    "train_model_mobilenet_flag = True\n",
    "test_model_mobilenet_flag = True\n",
    "\n",
    "train_model_InceptionResNetV2_flag = True\n",
    "test_model_InceptionResNetV2_flag = True\n",
    "\n",
    "save_matriz_pesos_flag = True\n",
    "\n",
    "# Numero de variables en el dataset de metadatos\n",
    "caract_metadata = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CF1_flag or CF4_flag:\n",
    "    # Diccionario con las classes CF1\n",
    "    classes = {0: ('bcc', 'CARCINOMA'),\n",
    "               1: ('df', 'DERMATOFIBROMA'),\n",
    "               2: ('bkl', 'LENTIGO'),\n",
    "               3: ('vasc', 'LESION_VASCULAR'),\n",
    "               4: ('mel', 'MELANOMA'),\n",
    "               5: ('nv', 'NEVUS'),\n",
    "               6: ('akiec', 'QUERATOSIS')}\n",
    "\n",
    "    cm_plot_classes = ['bcc', 'df', 'bkl', 'vasc', 'mel', 'nv', 'akiec']\n",
    "\n",
    "if CF2_flag or CF3_flag:\n",
    "    # Diccionario con las classes CF2\n",
    "    classes = {0: ('ben', 'BENIGNO'),\n",
    "               1: ('mal', 'MALIGNO')}\n",
    "\n",
    "    cm_plot_classes = ['ben', 'mal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"An%C3%A1lisis-exploratoria-de-datos\">Análisis exploratoria de datos<a class=\"anchor-link\" href=\"#An%C3%A1lisis-exploratoria-de-datos\">¶</a></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Funciones\">Funciones<a class=\"anchor-link\" href=\"#Funciones\">¶</a></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_normalizar_conjunto_imagenes(x, conjunto_nuevo):\n",
    "    for imagen in x:\n",
    "        # Convertir la imagen a un arreglo NumPy\n",
    "        img_array = np.array(imagen)\n",
    "        conjunto_nuevo.append(img_array)\n",
    "    conjunto_nuevo = 2 * (conjunto_nuevo - np.min(conjunto_nuevo)) / (np.max(conjunto_nuevo) - np.min(conjunto_nuevo)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modificar_resolucion_imagenes(x, conjunto_nuevo, anchura, altura):\n",
    "    resolucion = (anchura, altura)\n",
    "    for imagen in x:\n",
    "        imagen_pil = Image.open(path_img+imagen)\n",
    "        imagen2 = imagen_pil.convert(\"RGB\")\n",
    "        imagen_redimensionada = imagen2.resize(resolucion)\n",
    "        conjunto_nuevo.append(imagen_redimensionada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para la representación gráfica deL accuracy y loss de los conjuntos de entrenamiento y validación\n",
    "def plot_acc_loss(histfit):\n",
    "    \"\"\"\n",
    "    Función que representa las gráficas de accuracy y loss de los conjuntos de entrenamiento y validación.\n",
    "    \n",
    "    Parámetro de entrada:\n",
    "    histfit: histórico del entrenamiento y validación\n",
    "    \"\"\"\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 4)) # 2 gráficas en la misma fila \n",
    "    fig.suptitle('Accuracy y Loss del modelo')\n",
    "    sns.set_theme(style = 'darkgrid')\n",
    "    \n",
    "    # accuracy\n",
    "    ax1.plot(histfit['accuracy'])\n",
    "    ax1.plot(histfit['val_accuracy'])\n",
    "    ax1.set(xlabel = 'Epoch', ylabel = 'Accuracy')\n",
    "    ax1.legend(['train', 'valid'], loc = 'lower right')\n",
    "       \n",
    "    # loss\n",
    "    ax2.plot(histfit['loss'])\n",
    "    ax2.plot(histfit['val_loss'])\n",
    "    ax2.set(xlabel = 'Epoch', ylabel = 'Loss')\n",
    "    ax2.legend(['train','valid'], loc = \"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para representar la matriz de confusión\n",
    "def plot_confusion_matrix(cm, classes, normalize = False, cmap = plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    Función que representa la matriz de confusión.\n",
    "    Se puede obtener normalizada en porcentajes con: normalize = True.\n",
    "    \n",
    "    Parámetros de entrada:\n",
    "    cm: matriz de confusión\n",
    "    classes: array con las clases\n",
    "    normalize: boleano para indicar si se quieren los datos normalizados o no\n",
    "    cmap: paleta de colores\n",
    "    \n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n",
    "        title = 'Matriz de confusión normalizada en porcentajes'\n",
    "    else:\n",
    "        title = 'Matriz de confusión sin normalizar'\n",
    "        \n",
    "    plt.figure(figsize = (10,8))\n",
    "    plt.title(title, size = 18)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation = 45, size = 13)\n",
    "    plt.yticks(tick_marks, classes, size = 13)\n",
    "    fmt = '.2%' if normalize else 'd'\n",
    "    cm_df = pd.DataFrame(cm,\n",
    "                     index = classes, \n",
    "                     columns = classes)\n",
    "    sns.heatmap(cm_df, annot = True, fmt = fmt, cmap = cmap)\n",
    "    plt.ylabel('Realidad', size = 15)\n",
    "    plt.xlabel('Predicción', size = 15)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para representar la curva ROC multiclase\n",
    "def plot_multiclass_roc(model, X_test, y_test, classes, n_classes, figsize = (14, 7)):\n",
    "    \"\"\"\n",
    "    Función que devuelve una representación gráfica de la curva ROC multiclase.\n",
    "    \n",
    "    Parámetros de entrada:\n",
    "    model: modelo con el que se realiza la predicción\n",
    "    X_test: características del conjunto de test\n",
    "    y_test: etiquetas del conjunto de test\n",
    "    n_classes: entero con el número de clases\n",
    "    figsize: dimensiones del gráfico\n",
    "    \"\"\"\n",
    "    \n",
    "    # Predicción\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Diccionarios\n",
    "    fpr = dict()     # ratio de FP\n",
    "    tpr = dict()     # ratio de TP\n",
    "    roc_auc = dict() # curva ROC\n",
    "    \n",
    "    # Codificación one hot\n",
    "    y_test_dummies = pd.get_dummies(y_test, drop_first = False).values\n",
    "    \n",
    "    # Iteración por todas las clases\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = metrics.roc_curve(y_test_dummies[:, i], y_pred[:, i])\n",
    "        roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n",
    "        \n",
    "    # ROC para cada clase\n",
    "    fig, ax = plt.subplots(figsize = figsize)\n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('Ratio de falsos positivos')\n",
    "    ax.set_ylabel('Ratio de verdaderos positivos')\n",
    "    ax.set_title('CURVA ROC')\n",
    "    if CF1_flag or CF4_flag:\n",
    "        for i in range(n_classes):\n",
    "            ax.plot(fpr[i], tpr[i], label = 'Curva ROC (area = %0.2f) para la lesión cutánea %s' % (roc_auc[i], classes[i][1]))\n",
    "    if CF2_flag or CF3_flag:\n",
    "        ax.plot(fpr[0], tpr[0], label = 'Curva ROC (area = %0.2f) para la lesión cutánea %s' % (roc_auc[0], classes[0][1]))\n",
    "        ax.plot(fpr[1], tpr[1], label = 'Curva ROC (area = %0.2f) para la lesión cutánea %s' % (roc_auc[1], classes[1][1]))\n",
    "    ax.legend(loc = \"best\")\n",
    "    ax.grid(alpha = .4)\n",
    "    sns.despine()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que entrena, valida y guarda los resultados en el diccionario histfit\n",
    "def fn_train_val(histfit, model, x, y, batch_size, epochs, callbacks, x_val, y_val):\n",
    "    \"\"\"\n",
    "    Función que entrena y valida el modelo con el conjunto de datos, número de épocas y tamaño del batch introducidos\n",
    "    como parámetros de entrada.\n",
    "\n",
    "    histfit: diccionario para guardar los resultados del entrenamiento y validación\n",
    "    model: modelo a entrenar\n",
    "    x: conjunto de entrenamiento\n",
    "    y: etiquetas del conjunto de entrenamiento\n",
    "    batch_size: tamaño del batch\n",
    "    epochs: número de épocas   \n",
    "    callbacks: devolución de llamadas\n",
    "    x_val: \n",
    "    y_val = \n",
    "\n",
    "    \n",
    "    Devuelve:\n",
    "    histfit -- diccionario con los resultados del entrenamiento y validación\n",
    "    \"\"\"\n",
    "    \n",
    "    history = model.fit(x = x, y = y, batch_size = batch_size, epochs = epochs,\n",
    "                            callbacks = callbacks, validation_data=(x_val, y_val))\n",
    "\n",
    "    # Adición de los resultados al diccionario\n",
    "    for key, value in history.history.items():\n",
    "        for i in range(len(value)):\n",
    "            if key != 'lr':\n",
    "                histfit[key].append(value[i])\n",
    "    return histfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que entrena, valida y guarda los resultados en el diccionario histfit\n",
    "def fn_train_val2(histfit, model, x_rgb, x_metadata, y, val_data, batch_size, epochs, callbacks, steps_per_epoch = None, validation_steps = None, validation_batch_size = None):\n",
    "    \"\"\"\n",
    "    Función que entrena y valida el modelo con el conjunto de datos, número de épocas y tamaño del batch introducidos\n",
    "    como parámetros de entrada.\n",
    "\n",
    "    histfit: diccionario para guardar los resultados del entrenamiento y validación\n",
    "    model: modelo a entrenar\n",
    "    x: conjunto de entrenamiento\n",
    "    y: etiquetas del conjunto de entrenamiento\n",
    "    batch_size: tamaño del batch\n",
    "    epochs: número de épocas   \n",
    "    callbacks: devolución de llamadas\n",
    "    steps_per_epoch: pasos por época\n",
    "    validation_steps: pasos en la validación\n",
    "    validation_batch_size: tamaño del batch en la validación\n",
    "\n",
    "    \n",
    "    Devuelve:\n",
    "    histfit -- diccionario con los resultados del entrenamiento y validación\n",
    "    \"\"\"\n",
    "    \n",
    "    history = model.fit(x = [x_rgb,x_metadata], y = y, validation_data = val_data, batch_size = batch_size,\n",
    "                            epochs = epochs, callbacks = callbacks, steps_per_epoch = steps_per_epoch, \n",
    "                            validation_steps = validation_steps, validation_batch_size = validation_batch_size, class_weight = class_weights )\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Adición de los resultados al diccionario\n",
    "    for key, value in history.history.items():\n",
    "        for i in range(len(value)):\n",
    "            if key != 'lr':\n",
    "                histfit[key].append(value[i])\n",
    "    return histfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de clasificación de una imagen con un modelo determinado\n",
    "def class_img(model_key, image):\n",
    "    \"\"\"\n",
    "    Función que clasifica la imagen introducida con el modelo seleccionado.\n",
    "    \n",
    "    Parámetros de entrada:\n",
    "    model_key: clave del modelo. Diccionario: saved_models.\n",
    "    image: nombre del fichero que contiene la imagen a clasificar.\n",
    "    \n",
    "    Devuelve la clase predicha y el porcentaje de acierto de la clasificación.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Asignación del modelo\n",
    "    model = saved_models[model_key]\n",
    "        \n",
    "    # Ruta completa de la imagen a clasificar\n",
    "    img_path = image\n",
    "        \n",
    "    if model_key == 1:\n",
    "        width = WIDTH\n",
    "        height = HEIGHT\n",
    "    else:\n",
    "        width = WIDTH_224\n",
    "        height = HEIGHT_224\n",
    "        \n",
    "    # Obtención de la matriz de características de la imagen\n",
    "    img_arr = get_array_from_img(img_path, width, height)\n",
    "    # Dimensionamiento del conjunto de datos a las dimensiones esperada por el modelo\n",
    "    x = np.array(img_arr).reshape(-1, width, height, 3)\n",
    "            \n",
    "    # Normalización\n",
    "    x = (x - np.mean(x)) / np.std(x)\n",
    "        \n",
    "    # Clasificación de la imagen\n",
    "    result = model.predict(x)\n",
    "    class_pred = max(result[0])                                        # Clase predicha\n",
    "    class_ind = list(result[0]).index(class_pred)                      # Índice de la clase predicha\n",
    "    class_name = classes[class_ind][0] + ' · ' + classes[class_ind][1] # Descriptivo de la clase predicha\n",
    "    print('***** CLASIFICANDO CON MODELO ' + str(model_key) + '/' + str(len(saved_models)) + ' *****\\n')\n",
    "    print('\\tResultado de la clasificación: ' + class_name)\n",
    "    print('\\n\\tCerteza del resultado: ' + str(round(class_pred * 100, 3)) + '%\\n')\n",
    "    \n",
    "    return class_name, round(class_pred * 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para la obtención de la matriz de características de una imagen\n",
    "def get_array_from_img (img_path, width, height):\n",
    "    \"\"\"\n",
    "    Función que lee la imagen introducida como parámetro de entrada y la reescala a las dimensiones introducidas.\n",
    "    Devuelve una matriz de dimensiones width x height x 3 (RGB) con las características de la imagen.\n",
    "    \n",
    "    Parámetros de entrada:\n",
    "    img_path: ruta completa (incluido el nombre del fichero) de la imagen de entrada\n",
    "    width: ancho de la imagen de salida\n",
    "    height: alto de la imagen de salida\n",
    "    \n",
    "    Devuelve:\n",
    "    img_resized: matriz con las características de la imagen de entrada con dimensiones width x height x 3\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)        # lectura y conversión de formato BGR a RGB\n",
    "        dim = (width, height)\n",
    "        img_resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA) # reescalado de la imagen\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return img_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que devuelve una matriz con las características y etiquetas de las imágenes con las dimensiones introducidas\n",
    "def get_array_resized_images (width, height, range_ini, range_fin, path_img):\n",
    "    \"\"\"\n",
    "    Función que obtiene la matriz de características de las imágenes con las dimensiones introducidas.\n",
    "    \n",
    "    Parámetros de entrada:\n",
    "    width: ancho de la imagen de salida\n",
    "    height: alto de la imagen de salida\n",
    "    range_ini: nº de fila en la que empieza la selección\n",
    "    range_fin: nº de fila (no incluida) en la que termina la selección  \n",
    "    \n",
    "    Devuelve:\n",
    "    df_images_resized: matriz con las características de las imagénes con dimensiones:\n",
    "    (range_fin-range_ini) x ((width x height x 3) + 1 de la etiqueta correspondiente)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Array con los nombres de columnas de las dimensiones introducidas \n",
    "    columns = []\n",
    "    for i in range(width * height * 3):\n",
    "        columns.append('col_' + str(i))\n",
    "    columns.append('label')\n",
    "    \n",
    "    # Matriz para guardar las características y etiquetas de todas las imágenes\n",
    "    resized_images = []\n",
    "    \n",
    "    # Iteración de la tabla con los nombres de las imágenes\n",
    "    for index, row in metadata.iloc[range_ini:range_fin].iterrows():\n",
    "        # Obtención de la imagen matriz de características de la imagen en las nuevas dimensiones\n",
    "        img_arr = get_array_from_img(path_img + row['image'], width, height)\n",
    "        # Aplanado de la matriz a 1 dimensión\n",
    "        flattened_img = img_arr.flatten()\n",
    "        # Adición de la etiqueta de la imagen\n",
    "        flattened_img = np.append(flattened_img, row['label'])\n",
    "        # Adición del vector con las características de la imagen y etiqueta a la matriz con el resto de imágenes\n",
    "        resized_images.append(flattened_img)\n",
    "    \n",
    "    # Conversión a dataframe\n",
    "    df_resized_images = pd.DataFrame(resized_images, columns = columns) \n",
    "    \n",
    "    return df_resized_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función con la configuración de callbacks\n",
    "def def_callbacks(e_monitor = 'val_loss', e_mode = 'min', e_patience = 5, e_verbose = 2,\n",
    "                  r_monitor = 'val_accuracy', r_factor = 0.2, r_patience = 2, r_min_delta = 1e-4,\n",
    "                  r_min_lr = 1e-6, r_mode = 'max', r_verbose = 2, c_filepath = 'best_model.h5', \n",
    "                  c_monitor = 'val_accuracy', c_save_best_only = True, c_save_weights_only = True,\n",
    "                  c_mode = 'max', c_verbose = 2):\n",
    "    \"\"\"\n",
    "    Función que devuelve una lista con las opciones de callbacks\n",
    "    \n",
    "    Devuelve:\n",
    "    callbacks: lista con la configuración de la parada temprana, caida de la tasa de aprendizaje y punto de control\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Parada temprana en el caso de que la pérdida en la validación no mejore en 5 épocas\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor = e_monitor,\n",
    "                                                      mode = e_mode,\n",
    "                                                      patience = e_patience,\n",
    "                                                      verbose = e_verbose)\n",
    "    \n",
    "    # Caida de la tasa de aprendizaje después de 2 épocas sin mejora en el accuracy de la validación\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = r_monitor,\n",
    "                                                     factor = r_factor,\n",
    "                                                     patience = r_patience,\n",
    "                                                     min_delta = r_min_delta,\n",
    "                                                     min_lr = r_min_lr,\n",
    "                                                     mode = r_mode,\n",
    "                                                     verbose = r_verbose)\n",
    "    \n",
    "    # Guardado de los pesos del mejor modelo en función del mejor accuracy en la validación\n",
    "    checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath = c_filepath,\n",
    "                                                      monitor = c_monitor,\n",
    "                                                      save_best_only = c_save_best_only,\n",
    "                                                      save_weights_only = c_save_weights_only,\n",
    "                                                      mode = c_mode,\n",
    "                                                      verbose = c_verbose)\n",
    "    \n",
    "    # Array con los callbacks\n",
    "    callbacks = [early_stopping, checkpointer, reduce_lr]\n",
    "    \n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ic_class_image(image):\n",
    "    \n",
    "    \"\"\"\n",
    "    Clasifica una imagen con la Inteligencia Colectiva (IC).\n",
    "    \n",
    "    Parámetros:\n",
    "    image -- ruta completa de la imagen con el nombre del archivo que contiene la imagen.\n",
    "    \n",
    "    Devuelve:\n",
    "    class_id -- identificador numérico de la lesión (0-6)\n",
    "    lesion_skin_cod -- código de la lesión (akiec · bcc · bkl · df · mel · nv · vasc)\n",
    "    lesion_skin_desc -- descripción larga de la lesión\n",
    "    ic_accuracy -- porcentaje de exactitud del resultado\n",
    "    ic_prob -- vector con las probabilidades para cada una de las 7 lesiones\n",
    "    \"\"\"\n",
    "    \n",
    "    # Matriz para guardar los resultados de las predicciones de cada modelo para cada clase\n",
    "    if CF1_flag or CF4_flag:\n",
    "        m2 = pd.DataFrame(columns = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6'], dtype = np.float64)\n",
    "\n",
    "    if CF2_flag or CF3_flag:\n",
    "        m2 = pd.DataFrame(columns = ['C0', 'C1'], dtype = np.float64)\n",
    "    \n",
    "    # Vector para guardar los resultados de la IC\n",
    "    ic_result = []\n",
    "    # Vector para guardar la probalidad de cada clase de la IC\n",
    "    ic_prob = []\n",
    "    \n",
    "    # Vector para guardar el número de redes que han predicho la clase predichca por la IC\n",
    "    counted_pred = []\n",
    "    \n",
    "    # Predicción con cada modelo\n",
    "    for key in saved_models.keys():\n",
    "        model = saved_models[key]\n",
    "        if key == 1:\n",
    "            width = WIDTH\n",
    "            height = HEIGHT\n",
    "        else:\n",
    "            width = WIDTH_224\n",
    "            height = HEIGHT_224\n",
    "        \n",
    "        # Obtención de la matriz de características de la imagen\n",
    "        img_arr = get_array_from_img(image, width, height)\n",
    "        # Dimensionamiento del conjunto de datos a las dimensiones esperada por el modelo\n",
    "        x = np.array(img_arr).reshape(-1, width, height, 3)\n",
    "        # Normalización\n",
    "        x = (x - np.mean(x)) / np.std(x)\n",
    "        \n",
    "        # Clasificación de la imagen con el modelo\n",
    "        pred = model.predict(x, verbose=0)\n",
    "        \n",
    "        # Matriz temporal para guardar los resultados de las predicciones de este modelo\n",
    "        if CF1_flag or CF4_flag:\n",
    "            m_temp = pd.DataFrame(columns = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6'], dtype = np.float64)\n",
    "\n",
    "        if CF2_flag or CF3_flag:\n",
    "            m_temp = pd.DataFrame(columns = ['C0', 'C1'], dtype = np.float64)\n",
    "        \n",
    "        # Adición de los resultados a la matriz con los resultados del resto de redes\n",
    "        for i in range(len(classes)):\n",
    "            m_temp['C'+str(i)] = [round(pred[0][i],5)]\n",
    "            \n",
    "        m2 = m2.append(m_temp)\n",
    "        \n",
    "    # Reinicio de los índices\n",
    "    m2 = m2.reset_index()\n",
    "    # Borrado de la columna index\n",
    "    m2 = m2.drop(['index'], axis = 1)\n",
    "    # Multiplicación de la matriz de pesos con la matriz de predicciones de la imagen\n",
    "    m3 = m2.multiply(m1)\n",
    "    print('Accuracy de cada modelo para cada clase (pesos generales)')\n",
    "    print(m1)\n",
    "    print('Accuracy de cada modelo para cada clase (predicciones de la imagen)')\n",
    "    print(m2)\n",
    "    print('Accuracy de cada modelo para cada clase (predicciones ponderadas de la imagen)')\n",
    "    print(m3)\n",
    "    # Guardado de los resultados de la inteligencia colectiva como la suma de valores de todas las redes\n",
    "    for i in range(len(classes)):\n",
    "        ic_result.append(m3['C' + str(i)].sum())\n",
    "    \n",
    "    # Probabilidad de cada clase\n",
    "    for i in range(len(ic_result)):\n",
    "        ic_prob.append(ic_result[i] / sum(ic_result))\n",
    "    \n",
    "    # Obtención del id de la clase predicha\n",
    "    class_id = ic_result.index(max(ic_result))\n",
    "    # Máximo marcador\n",
    "    max_score = max(ic_result)\n",
    "        \n",
    "    # Búsqueda de la cantidad de redes que han predicho la clase predicha por la IC\n",
    "    for i in range(len(saved_models)):\n",
    "        counted_pred.append(m3.iloc[i].argmax())\n",
    "        \n",
    "    votes_pred = counted_pred.count(class_id)\n",
    "    # Accuracy de la IC como la máxima puntuación entre las redes que han votado la clase predicha\n",
    "    ic_accuracy = round(max_score / votes_pred, 2)\n",
    "    # Código de la lesión cutánea\n",
    "    lesion_skin_cod = classes[class_id][0]\n",
    "    # Literal de la lesión cutánea    \n",
    "    lesion_skin_desc = classes[class_id][1]  \n",
    "            \n",
    "    return class_id, lesion_skin_cod, lesion_skin_desc, ic_accuracy, ic_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Entrenamientos\">Entrenamientos<a class=\"anchor-link\" href=\"#Entrenamientos\">¶</a></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz para guardar los pesos del accuracy de cada modelo para cada clase\n",
    "if CF1_flag or CF4_flag:\n",
    "    m1 = pd.DataFrame(columns = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6'], dtype = np.float64) \n",
    "\n",
    "if CF2_flag or CF3_flag:\n",
    "    m1 = pd.DataFrame(columns = ['C0', 'C1'], dtype = np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de los conjuntos de train y test\n",
    "x_train = np.load(output_path + 'x_train.npy')\n",
    "x_test = np.load(output_path + 'x_test.npy')\n",
    "y_train = np.load(output_path + 'y_train.npy')\n",
    "y_test = np.load(output_path + 'y_test.npy')\n",
    "x_val = np.load(output_path + 'x_val.npy')\n",
    "y_val = np.load(output_path + 'y_val.npy')\n",
    "\n",
    "x_train_ext = np.load(output_path + 'x_train_ext.npy')\n",
    "x_test_ext = np.load(output_path + 'x_test_ext.npy')\n",
    "x_val_ext = np.load(output_path + 'x_val_ext.npy')\n",
    "\n",
    "# Definimos la validation data para un modelo con multiples inputs\n",
    "validation_data =  ([x_val,x_val_ext], y_val)\n",
    "\n",
    "print(\"Forma de x_train:\", x_train.shape)\n",
    "print(\"Forma de x_train_ext:\", x_train_ext.shape)\n",
    "print(type(x_train))\n",
    "print(type(x_train_ext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "y_val = y_val.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Modelo-1-%C2%B7-CNN-adhoc\">Modelo 1 · CNN adhoc<a class=\"anchor-link\" href=\"#Modelo-1-%C2%B7-CNN-adhoc\">¶</a></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Modelo-2-%C2%B7-Red-transformer-ViT-B16\">Modelo 2 · Red transformer ViT-B16<a class=\"anchor-link\" href=\"#Modelo-2-%C2%B7-Red-transformer-ViT-B16\">¶</a></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Modelo-3-%C2%B7-Swin-Transformer\">Modelo 3 · Swin Transformer<a class=\"anchor-link\" href=\"#Modelo-3-%C2%B7-Swin-Transformer\">¶</a></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Modelo-4-%C2%B7-Red-EfficientNet-B0\">Modelo 4 · Red EfficientNet B0<a class=\"anchor-link\" href=\"#Modelo-4-%C2%B7-Red-EfficientNet-B0\">¶</a></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_EfficientNet():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 7 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = keras.layers.Input(shape = (224, 224, 3))\n",
    "    model = EfficientNetB0(include_top = False,                         # no incluir la capa de clasificación\n",
    "                           input_tensor = inputs, \n",
    "                           weights = 'imagenet')                        # preentrenado con el dataset imagenet.\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    model.trainable = True\n",
    "    \n",
    "    \n",
    "    model = Sequential([model, \n",
    "                      keras.layers.AveragePooling2D(pool_size = (2 ,2)),      \n",
    "                      keras.layers.Flatten(), \n",
    "                      keras.layers.Dense(64, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.2), \n",
    "                      keras.layers.Dense(16, activation = 'relu'),\n",
    "                      keras.layers.Dense(7, activation = 'softmax'),\n",
    "  ])\n",
    " \n",
    "    return model\n",
    "def build_model_EfficientNet_binary():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 2 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    # Define the input layer\n",
    "    input_rgb = layers.Input(shape=(224, 224, 3), name='input_rgb')\n",
    "    \n",
    "    # Create the EfficientNet B0 model without the top classification layers\n",
    "    base_model = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=input_rgb)\n",
    "\n",
    "    # Freeze the base model\n",
    "    base_model.trainable = True\n",
    "\n",
    "    # Get the output of the base model\n",
    "    x = base_model(input_rgb)\n",
    "    \n",
    "    # Add additional layers to the base model output\n",
    "    x = keras.layers.AveragePooling2D(pool_size = (2 ,2))(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(64, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    x = keras.layers.Dense(16, activation='relu')(x)\n",
    "    \n",
    "    input_metadata = layers.Input(shape=(caract_metadata,), name='input_metadata')\n",
    "    x_final = keras.layers.Concatenate()([x, input_metadata])\n",
    "    \n",
    "    outputs = keras.layers.Dense(2, activation='softmax')(x_final)\n",
    "    \n",
    "    # Create the model\n",
    "    model = models.Model(inputs=[input_rgb, input_metadata], outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_EfficientNet_flag == True:\n",
    "    # Mostrar estructura de la red\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_EfficientNet()\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_EfficientNet_binary()\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_EfficientNet_flag == True:\n",
    "    # Hiperparámetros\n",
    "    # epochs\n",
    "    n_epochs = [50]\n",
    "    # batch sizes\n",
    "    batch_sizes = [8, 16, 32,64]\n",
    "    # learning rates\n",
    "    learning_rates = [0.001, 0.0005, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "print(class_weights)\n",
    "\n",
    "class_weights = {0: .74, 1: 1.54}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_EfficientNet_flag == True:\n",
    "    # Diccionario para almacenar los resultados del entrenamiento\n",
    "    history4 = {'epochs':[], 'batch_size':[], 'learning_rate':[],'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[], 'elapsed_time':[]}\n",
    "\n",
    "    # Variable per guardar el mejor accuracy obtenido con el conjunto de validación\n",
    "    best_val_acc = 0\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = def_callbacks()\n",
    "\n",
    "    # Iterar por todas las combinaciones de hiperparámetros\n",
    "    print('Entrenando el modelo EfficientNet B0...')\n",
    "    for epochs in n_epochs:\n",
    "        for batch_size in batch_sizes:\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "            valid_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "            \n",
    "            train_dataset = train_dataset.batch(batch_size)\n",
    "            valid_dataset = valid_dataset.batch(batch_size)\n",
    "            for learning_rate in learning_rates:\n",
    "                # Diccionario para guardar los resultados de un entrenamiento\n",
    "                histfit = {'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[]}\n",
    "                print('\\tHiperparámetros: epochs: {} | batch_size: {} | learning_rate: {}'.format(epochs, batch_size, learning_rate))\n",
    "                # Creación y compilación del modelo\n",
    "                if CF1_flag or CF4_flag:\n",
    "                    model = build_model_EfficientNet()\n",
    "                if CF2_flag or CF3_flag:\n",
    "                    model = build_model_EfficientNet_binary()\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "                if CF1_flag or CF4_flag:\n",
    "                    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "                if CF2_flag or CF3_flag:\n",
    "                    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])    \n",
    "                steps_per_epoch = np.ceil(len(x_train) / batch_size)\n",
    "                validation_steps = np.ceil(len(x_test) / batch_size)\n",
    "                validation_batch_size = batch_size\n",
    "                # Con Data Augmentation\n",
    "                validation_split = 0.0\n",
    "                # Cómputo del tiempo transcurrido durante el entrenamiento y validación\n",
    "                elapsed_time = timeit.timeit('fn_train_val2(histfit, model, x_train, x_train_ext, y_train, validation_data, batch_size, epochs,'+\n",
    "                                             'callbacks)', globals = globals(), number = 1)                     \n",
    "                # Adición de los resultados al diccionario history4\n",
    "                history4['epochs'].append(epochs)\n",
    "                history4['batch_size'].append(batch_size)\n",
    "                history4['learning_rate'].append(learning_rate)\n",
    "                # Obtenemos el índice con el accuracy máximo con el conjunto de validación \n",
    "                max_val_acc_idx = histfit['val_accuracy'].index(max(histfit['val_accuracy']))\n",
    "                history4['loss'].append(histfit['loss'][max_val_acc_idx])\n",
    "                history4['accuracy'].append(histfit['accuracy'][max_val_acc_idx])\n",
    "                history4['val_loss'].append(histfit['val_loss'][max_val_acc_idx])\n",
    "                history4['val_accuracy'].append(histfit['val_accuracy'][max_val_acc_idx])\n",
    "                history4['elapsed_time'].append(elapsed_time)\n",
    "\n",
    "                # Guardamos el modelo con mejor accuracy en la validación\n",
    "                if histfit['val_accuracy'][max_val_acc_idx] > best_val_acc:\n",
    "                    best_val_acc = histfit['val_accuracy'][max_val_acc_idx]\n",
    "                    df_histfit = pd.DataFrame.from_dict(histfit)\n",
    "                    df_histfit.to_pickle(output_path + 'df_histfit4')\n",
    "                    model.save(output_path + 'efficientNet_best_model.h5')\n",
    "\n",
    "    # Conversión del dataframe a diccionario\n",
    "    df_history4 = pd.DataFrame.from_dict(history4)\n",
    "\n",
    "    # Guardado del dataframe con los resultados de todos los entrenamientos\n",
    "    df_history4.to_pickle(output_path + 'df_history4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model_EfficientNet_flag == True:\n",
    "    # Recuperación del dataframe con los resultados de los entrenamientos con todas las combinaciones de hiperparámetros definidas\n",
    "    df_history4 = pd.read_pickle(output_path + 'df_history4')\n",
    "\n",
    "    # Obtención de las 5 combinaciones de hiperparámetros que han devuelto mejor accuracy en la validación  \n",
    "    df_history4.sort_values(by = 'val_accuracy', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model_EfficientNet_flag == True:\n",
    "    # Recuperación del dataframe con los resultados del entramiento con los hiperparámetros que han devuelto mejor resultados en la validación\n",
    "    df_histfit4 = pd.read_pickle(output_path + 'df_histfit4')\n",
    "\n",
    "    # Representación de las gráficas con el accuracy y loss del mejor modelo obtenido\n",
    "    plot_acc_loss(df_histfit4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model_EfficientNet_flag == True:\n",
    "    # Evaluación del modelo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_EfficientNet()\n",
    "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_EfficientNet_binary()\n",
    "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.load_weights(output_path + 'efficientNet_best_model.h5')\n",
    "    model.save(output_path + 'model4.h5') # Guardado completo del modelo\n",
    "\n",
    "    loss, acc = model.evaluate([x_test,x_test_ext], y_test, verbose = 2) # Evaluación con el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model_EfficientNet_flag == True:\n",
    "    # Carga del modelo completo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_EfficientNet()\n",
    "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_EfficientNet_binary()\n",
    "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.load_weights(output_path + 'model4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model_EfficientNet_flag == True:\n",
    "    # Predicción con el conjunto de test\n",
    "    y_pred = model.predict([x_test,x_test_ext])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model_EfficientNet_flag == True:\n",
    "    # Curva ROC multiclase\n",
    "    plot_multiclass_roc(model, [x_test,x_test_ext], y_test, classes, len(classes), figsize = (14, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model_EfficientNet_flag == True:\n",
    "    # Matriz de confusión\n",
    "    cm = confusion_matrix(y_test, y_pred.argmax(axis = 1))\n",
    "\n",
    "    # Cálculo de la matriz de confusión y representación normalizada y sin normalizar\n",
    "    plot_confusion_matrix(cm, cm_plot_classes, normalize = True)\n",
    "    plot_confusion_matrix(cm, cm_plot_classes, normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model_EfficientNet_flag == True:\n",
    "    # Métricas\n",
    "    print('***** MÉTRICAS *****')\n",
    "    print('Accuracy: {:.5f}'.format(metrics.accuracy_score(y_test, y_pred.argmax(axis = 1))))\n",
    "    print('Precision: {:.5f}'.format(metrics.precision_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "    print('Recall: {:.5f}'.format(metrics.recall_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "    print('F1 Score: {:.5f}'.format(metrics.f1_score(y_test, y_pred.argmax(axis = 1), average ='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model_EfficientNet_flag == True:\n",
    "    # Dataframe temporal para guardar los accuracies de cada clase de este modelo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        m_temp = pd.DataFrame(columns = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6'], dtype = np.float64) \n",
    "    if CF2_flag or CF3_flag:\n",
    "        m_temp = pd.DataFrame(columns = ['C0', 'C1'], dtype = np.float64)\n",
    "    for i in range(len(classes)):\n",
    "        m_temp['C'+str(i)] = [round(cm[i][i]/np.sum(cm[i]),5)]\n",
    "\n",
    "    # Adición de la fila con el accuracy de cada clase de este modelo a la matriz con el resto de modelos\n",
    "    m1 = pd.concat([m1, m_temp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Modelo-5-%C2%B7-Red-Xception\">Modelo 5 · Red Xception<a class=\"anchor-link\" href=\"#Modelo-5-%C2%B7-Red-Xception\">¶</a></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_Xception():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 7 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = keras.layers.Input(shape = (224, 224, 3))\n",
    "    model = Xception(include_top = False,                         # no incluir la capa de clasificación\n",
    "                           input_tensor = inputs, \n",
    "                           weights = 'imagenet')                  # preentrenado con el dataset imagenet.\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    model.trainable = True\n",
    "    \n",
    "    \n",
    "    model = Sequential([model, \n",
    "                      keras.layers.AveragePooling2D(pool_size = (2 ,2)),      \n",
    "                      keras.layers.Flatten(), \n",
    "                      keras.layers.Dense(64, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.2), \n",
    "                      keras.layers.Dense(16, activation = 'relu'),\n",
    "                      keras.layers.Dense(7, activation = 'softmax'),\n",
    "  ])\n",
    " \n",
    "    return model\n",
    "def build_model_Xception_binary():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 2 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input layer\n",
    "    input_rgb = layers.Input(shape=(224, 224, 3), name='input_rgb')\n",
    "    \n",
    "    # Create the Xception model without the top classification layers\n",
    "    base_model = Xception(include_top=False, weights='imagenet', input_tensor=input_rgb)\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    base_model.trainable = True\n",
    "\n",
    "    # Get the output of the base model\n",
    "    x = base_model(input_rgb)\n",
    "    \n",
    "    # Add additional layers to the base model output\n",
    "    x = keras.layers.AveragePooling2D(pool_size = (2 ,2))(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(64, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    x = keras.layers.Dense(16, activation='relu')(x)\n",
    "    \n",
    "    input_metadata = layers.Input(shape=(caract_metadata,), name='input_metadata')\n",
    "    x_final = keras.layers.Concatenate()([x, input_metadata])\n",
    "    \n",
    "    outputs = keras.layers.Dense(2, activation='softmax')(x_final)\n",
    "    \n",
    "    # Create the model\n",
    "    model = models.Model(inputs=[input_rgb, input_metadata], outputs=outputs)\n",
    "     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_xception_flag == True:\n",
    "    # Mostrar estructura de la red\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_Xception()\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_Xception_binary()\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_xception_flag == True:\n",
    "    # Hiperparámetros\n",
    "    # epochs\n",
    "    n_epochs = [50]\n",
    "    # batch sizes\n",
    "    batch_sizes = [8, 16, 32,64]\n",
    "    # learning rates\n",
    "    learning_rates = [0.001, 0.0005, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_xception_flag == True:\n",
    "    # Diccionario para almacenar los resultados del entrenamiento\n",
    "    history5 = {'epochs':[], 'batch_size':[], 'learning_rate':[],'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[], 'elapsed_time':[]}\n",
    "\n",
    "    # Variable per guardar el mejor accuracy obtenido con el conjunto de validación\n",
    "    best_val_acc = 0\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = def_callbacks()\n",
    "\n",
    "    # Iterar por todas las combinaciones de hiperparámetros\n",
    "    print('Entrenando el modelo Xception...')\n",
    "    for epochs in n_epochs:\n",
    "        for batch_size in batch_sizes:\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "            valid_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "            \n",
    "            train_dataset = train_dataset.batch(batch_size)\n",
    "            valid_dataset = valid_dataset.batch(batch_size)\n",
    "            for learning_rate in learning_rates:\n",
    "                # Diccionario para guardar los resultados de un entrenamiento\n",
    "                histfit = {'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[]}\n",
    "                print('\\tHiperparámetros: epochs: {} | batch_size: {} | learning_rate: {}'.format(epochs, batch_size, learning_rate))\n",
    "                # Creación y compilación del modelo\n",
    "                if CF1_flag or CF4_flag:\n",
    "                    model = build_model_Xception()\n",
    "                if CF2_flag or CF3_flag:\n",
    "                    model = build_model_Xception_binary()\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "                if CF1_flag or CF4_flag:\n",
    "                    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "                if CF2_flag or CF3_flag:\n",
    "                    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])       \n",
    "                steps_per_epoch = np.ceil(len(x_train) / batch_size)\n",
    "                validation_steps = np.ceil(len(x_test) / batch_size)\n",
    "                validation_batch_size = batch_size\n",
    "                # Con Data Augmentation\n",
    "                validation_split = 0.0\n",
    "                # Cómputo del tiempo transcurrido durante el entrenamiento y validación\n",
    "                elapsed_time = timeit.timeit('fn_train_val2(histfit, model, x_train, x_train_ext, y_train, validation_data, batch_size, epochs,'+\n",
    "                                             'callbacks)', globals = globals(), number = 1)           \n",
    "                # Adición de los resultados al diccionario history5\n",
    "                history5['epochs'].append(epochs)\n",
    "                history5['batch_size'].append(batch_size)\n",
    "                history5['learning_rate'].append(learning_rate)\n",
    "                # Obtenemos el índice con el accuracy máximo con el conjunto de validación \n",
    "                max_val_acc_idx = histfit['val_accuracy'].index(max(histfit['val_accuracy']))\n",
    "                history5['loss'].append(histfit['loss'][max_val_acc_idx])\n",
    "                history5['accuracy'].append(histfit['accuracy'][max_val_acc_idx])\n",
    "                history5['val_loss'].append(histfit['val_loss'][max_val_acc_idx])\n",
    "                history5['val_accuracy'].append(histfit['val_accuracy'][max_val_acc_idx])\n",
    "                history5['elapsed_time'].append(elapsed_time)\n",
    "\n",
    "                # Guardamos el modelo con mejor accuracy en la validación\n",
    "                if histfit['val_accuracy'][max_val_acc_idx] > best_val_acc:\n",
    "                    best_val_acc = histfit['val_accuracy'][max_val_acc_idx]\n",
    "                    df_histfit = pd.DataFrame.from_dict(histfit)\n",
    "                    df_histfit.to_pickle(output_path + 'df_histfit5')\n",
    "                    model.save(output_path + 'Xception_best_model.h5')\n",
    "\n",
    "    # Conversión del dataframe a diccionario\n",
    "    df_history5 = pd.DataFrame.from_dict(history5)\n",
    "\n",
    "    # Guardado del dataframe con los resultados de todos los entrenamientos\n",
    "    df_history5.to_pickle(output_path + 'df_history5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_xception_flag == True:\n",
    "    # Recuperación del dataframe con los resultados de los entrenamientos con todas las combinaciones de hiperparámetros definidas\n",
    "    df_history5 = pd.read_pickle(output_path + 'df_history5')\n",
    "\n",
    "    # Obtención de las 5 combinaciones de hiperparámetros que han devuelto mejor accuracy en la validación  \n",
    "    df_history5.sort_values(by = 'val_accuracy', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_xception_flag == True:\n",
    "    # Recuperación del dataframe con los resultados del entramiento con los hiperparámetros que han devuelto mejor resultados en la validación\n",
    "    df_histfit5 = pd.read_pickle(output_path + 'df_histfit5')\n",
    "\n",
    "    # Representación de las gráficas con el accuracy y loss del mejor modelo obtenido\n",
    "    plot_acc_loss(df_histfit5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model_xception_flag == True:\n",
    "    # Evaluación del modelo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_Xception()\n",
    "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_Xception_binary()\n",
    "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.load_weights(output_path + 'Xception_best_model.h5')\n",
    "    model.save(output_path + 'model5.h5') # Guardado completo del modelo\n",
    "\n",
    "    loss, acc = model.evaluate([x_test,x_test_ext], y_test, verbose = 2) # Evaluación con el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_xception_flag == True:\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_Xception()\n",
    "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_Xception_binary()\n",
    "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.load_weights(output_path + 'model5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_xception_flag == True:\n",
    "    # Predicción con el conjunto de test\n",
    "    y_pred = model.predict([x_test,x_test_ext])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_xception_flag == True:\n",
    "    # Curva ROC multiclase\n",
    "    plot_multiclass_roc(model, [x_test,x_test_ext], y_test, classes, len(classes), figsize = (14, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_xception_flag == True:\n",
    "    # Matriz de confusión\n",
    "    cm = confusion_matrix(y_test, y_pred.argmax(axis = 1))\n",
    "\n",
    "    # Cálculo de la matriz de confusión y representación normalizada y sin normalizar\n",
    "    plot_confusion_matrix(cm, cm_plot_classes, normalize = True)\n",
    "    plot_confusion_matrix(cm, cm_plot_classes, normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_xception_flag == True:\n",
    "    # Métricas\n",
    "    print('***** MÉTRICAS *****')\n",
    "    print('Accuracy: {:.5f}'.format(metrics.accuracy_score(y_test, y_pred.argmax(axis = 1))))\n",
    "    print('Precision: {:.5f}'.format(metrics.precision_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "    print('Recall: {:.5f}'.format(metrics.recall_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "    print('F1 Score: {:.5f}'.format(metrics.f1_score(y_test, y_pred.argmax(axis = 1), average ='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_xception_flag == True:\n",
    "    # Dataframe temporal para guardar los accuracies de cada clase de este modelo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        m_temp = pd.DataFrame(columns = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6'], dtype = np.float64) \n",
    "    if CF2_flag or CF3_flag:\n",
    "        m_temp = pd.DataFrame(columns = ['C0', 'C1'], dtype = np.float64)\n",
    "    for i in range(len(classes)):\n",
    "        m_temp['C'+str(i)] = [round(cm[i][i]/np.sum(cm[i]),5)]\n",
    "\n",
    "    # Adición de la fila con el accuracy de cada clase de este modelo a la matriz con el resto de modelos\n",
    "    m1 = pd.concat([m1, m_temp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Modelo-6-%C2%B7-ResNet152\">Modelo 6 · ResNet152<a class=\"anchor-link\" href=\"#Modelo-6-%C2%B7-ResNet152\">¶</a></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_ResNet152():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 7 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = keras.layers.Input(shape = (224, 224, 3))\n",
    "    model = ResNet152(include_top = False,                   # no incluir la capa de clasificación\n",
    "                      input_tensor = inputs,\n",
    "                      weights = 'imagenet')                  # preentrenado con el dataset imagenet.\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    model.trainable = True\n",
    "    \n",
    "    model = Sequential([model, \n",
    "                      keras.layers.AveragePooling2D(pool_size = (2 ,2)),      \n",
    "                      keras.layers.Flatten(), \n",
    "                      keras.layers.Dense(64, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.2), \n",
    "                      keras.layers.Dense(16, activation = 'relu'),\n",
    "                      keras.layers.Dense(2, activation = 'softmax'),\n",
    "  ])\n",
    " \n",
    "    return model\n",
    "def build_model_ResNet152_binary():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 2 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input layer\n",
    "    input_rgb = layers.Input(shape=(224, 224, 3), name='input_rgb')\n",
    "    \n",
    "    # Create the ResNet152 model without the top classification layers\n",
    "    base_model = ResNet152(include_top=False, weights='imagenet', input_tensor=input_rgb)\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    base_model.trainable = True\n",
    "\n",
    "    # Get the output of the base model\n",
    "    x = base_model(input_rgb)\n",
    "    \n",
    "    # Add additional layers to the base model output\n",
    "    x = keras.layers.AveragePooling2D(pool_size = (2 ,2))(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(64, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    x = keras.layers.Dense(16, activation='relu')(x)\n",
    "    \n",
    "    input_metadata = layers.Input(shape=(caract_metadata,), name='input_metadata')\n",
    "    x_final = keras.layers.Concatenate()([x, input_metadata])\n",
    "    \n",
    "    outputs = keras.layers.Dense(2, activation='softmax')(x_final)\n",
    "    \n",
    "    # Create the model\n",
    "    model = models.Model(inputs=[input_rgb, input_metadata], outputs=outputs)\n",
    "     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_resnet_flag == True:\n",
    "    # Mostrar estructura de la red\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_ResNet152()\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_ResNet152_binary()\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_resnet_flag == True:\n",
    "    # Hiperparámetros\n",
    "    # epochs\n",
    "    n_epochs = [50]\n",
    "    # batch sizes\n",
    "    batch_sizes = [8, 16, 32]\n",
    "    # learning rates\n",
    "    learning_rates = [0.0005, 0.0001, 0.00005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_resnet_flag == True:\n",
    "    # Diccionario para almacenar los resultados del entrenamiento\n",
    "    history6 = {'epochs':[], 'batch_size':[], 'learning_rate':[],'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[], 'elapsed_time':[]}\n",
    "\n",
    "    # Variable per guardar el mejor accuracy obtenido con el conjunto de validación\n",
    "    best_val_acc = 0\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = def_callbacks()\n",
    "\n",
    "    # Iterar por todas las combinaciones de hiperparámetros\n",
    "    print('Entrenando el modelo ResNet152...')\n",
    "    for epochs in n_epochs:\n",
    "        for batch_size in batch_sizes:\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "            valid_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "            \n",
    "            train_dataset = train_dataset.batch(batch_size)\n",
    "            valid_dataset = valid_dataset.batch(batch_size)\n",
    "            for learning_rate in learning_rates:\n",
    "                # Diccionario para guardar los resultados de un entrenamiento\n",
    "                histfit = {'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[]}\n",
    "                print('\\tHiperparámetros: epochs: {} | batch_size: {} | learning_rate: {}'.format(epochs, batch_size, learning_rate))\n",
    "                # Creación y compilación del modelo\n",
    "                if CF1_flag or CF4_flag:\n",
    "                    model = build_model_ResNet152()\n",
    "                if CF2_flag or CF3_flag:\n",
    "                    model = build_model_ResNet152_binary()\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "                if CF1_flag or CF4_flag:\n",
    "                    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "                if CF2_flag or CF3_flag:\n",
    "                    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])   \n",
    "                steps_per_epoch = np.ceil(len(x_train) / batch_size)\n",
    "                validation_steps = np.ceil(len(x_test) / batch_size)\n",
    "                validation_batch_size = batch_size\n",
    "                # Con Data Augmentation\n",
    "                validation_split = 0.0\n",
    "                # Cómputo del tiempo transcurrido durante el entrenamiento y validación\n",
    "                elapsed_time = timeit.timeit('fn_train_val2(histfit, model, x_train, x_train_ext, y_train, validation_data, batch_size, epochs,'+\n",
    "                                             'callbacks)', globals = globals(), number = 1)    \n",
    "\n",
    "                # Adición de los resultados al diccionario history6\n",
    "                history6['epochs'].append(epochs)\n",
    "                history6['batch_size'].append(batch_size)\n",
    "                history6['learning_rate'].append(learning_rate)\n",
    "                # Obtenemos el índice con el accuracy máximo con el conjunto de validación \n",
    "                max_val_acc_idx = histfit['val_accuracy'].index(max(histfit['val_accuracy']))\n",
    "                history6['loss'].append(histfit['loss'][max_val_acc_idx])\n",
    "                history6['accuracy'].append(histfit['accuracy'][max_val_acc_idx])\n",
    "                history6['val_loss'].append(histfit['val_loss'][max_val_acc_idx])\n",
    "                history6['val_accuracy'].append(histfit['val_accuracy'][max_val_acc_idx])\n",
    "                history6['elapsed_time'].append(elapsed_time)\n",
    "\n",
    "                # Guardamos el modelo con mejor accuracy en la validación\n",
    "                if histfit['val_accuracy'][max_val_acc_idx] > best_val_acc:\n",
    "                    best_val_acc = histfit['val_accuracy'][max_val_acc_idx]\n",
    "                    df_histfit = pd.DataFrame.from_dict(histfit)\n",
    "                    df_histfit.to_pickle(output_path + 'df_histfit6')\n",
    "                    model.save(output_path + 'ResNet152_best_model.h5')\n",
    "\n",
    "    # Conversión del dataframe a diccionario\n",
    "    df_history6 = pd.DataFrame.from_dict(history6)\n",
    "\n",
    "    # Guardado del dataframe con los resultados de todos los entrenamientos\n",
    "    df_history6.to_pickle(output_path + 'df_history6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_resnet_flag == True:\n",
    "    # Recuperación del dataframe con los resultados de los entrenamientos con todas las combinaciones de hiperparámetros definidas\n",
    "    df_history6 = pd.read_pickle(output_path + 'df_history6')\n",
    "    \n",
    "    # Obtención de las 5 combinaciones de hiperparámetros que han devuelto mejor accuracy en la validación  \n",
    "    df_history6.sort_values(by = 'val_accuracy', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_resnet_flag == True:\n",
    "    # Recuperación del dataframe con los resultados del entramiento con los hiperparámetros que han devuelto mejor resultados en la validación\n",
    "    df_histfit6 = pd.read_pickle(output_path + 'df_histfit6')\n",
    "    \n",
    "    # Representación de las gráficas con el accuracy y loss del mejor modelo obtenido\n",
    "    plot_acc_loss(df_histfit6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model_resnet_flag == True:\n",
    "    # Evaluación del modelo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_ResNet152()\n",
    "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_ResNet152_binary()\n",
    "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.load_weights(output_path + 'ResNet152_best_model.h5')\n",
    "    #model.save(output_path + 'model6.h5') # guardado del modelo completo\n",
    "    \n",
    "    loss, acc = model.evaluate([x_test,x_test_ext], y_test, verbose = 2) # Evaluación con el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_resnet_flag == True:\n",
    "    # Carga del modelo completo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_ResNet152()\n",
    "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_ResNet152_binary()\n",
    "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.load_weights(output_path + 'model6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_resnet_flag == True:  \n",
    "    # Predicción con el conjunto de test\n",
    "    y_pred = model.predict([x_test,x_test_ext])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_resnet_flag == True:\n",
    "    # Curva ROC multiclase\n",
    "    plot_multiclass_roc(model, [x_test,x_test_ext], y_test, classes, len(classes), figsize = (14, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_resnet_flag == True:\n",
    "    # Matriz de confusión\n",
    "    cm = confusion_matrix(y_test, y_pred.argmax(axis = 1))\n",
    "    \n",
    "    # Cálculo de la matriz de confusión y representación normalizada y sin normalizar\n",
    "    plot_confusion_matrix(cm, cm_plot_classes, normalize = True)\n",
    "    plot_confusion_matrix(cm, cm_plot_classes, normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_resnet_flag == True:\n",
    "    # Métricas\n",
    "    print('***** MÉTRICAS *****')\n",
    "    print('Accuracy: {:.5f}'.format(metrics.accuracy_score(y_test, y_pred.argmax(axis = 1))))\n",
    "    print('Precision: {:.5f}'.format(metrics.precision_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "    print('Recall: {:.5f}'.format(metrics.recall_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "    print('F1 Score: {:.5f}'.format(metrics.f1_score(y_test, y_pred.argmax(axis = 1), average ='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_resnet_flag == True:    \n",
    "    # Dataframe temporal para guardar los accuracies de cada clase de este modelo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        m_temp = pd.DataFrame(columns = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6'], dtype = np.float64) \n",
    "    if CF2_flag or CF3_flag:\n",
    "        m_temp = pd.DataFrame(columns = ['C0', 'C1'], dtype = np.float64)\n",
    "    for i in range(len(classes)):\n",
    "        m_temp['C'+str(i)] = [round(cm[i][i]/np.sum(cm[i]),5)]\n",
    "        \n",
    "    # Adición de la fila con el accuracy de cada clase de este modelo a la matriz con el resto de modelos\n",
    "    m1 = pd.concat([m1, m_temp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Model-7-%C2%B7-VGG16\">Model 7 · VGG16<a class=\"anchor-link\" href=\"#Model-7-%C2%B7-VGG16\">¶</a></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_VGG16():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 7 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = keras.layers.Input(shape = (224, 224, 3))\n",
    "    model = VGG16(include_top = False,                   # no incluir la capa de clasificación\n",
    "                      input_tensor = inputs,\n",
    "                      weights = 'imagenet')                  # preentrenado con el dataset imagenet.\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    model.trainable = True\n",
    "    \n",
    "    model = Sequential([model, \n",
    "                      keras.layers.AveragePooling2D(pool_size = (2 ,2)),      \n",
    "                      keras.layers.Flatten(), \n",
    "                      keras.layers.Dense(64, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.2), \n",
    "                      keras.layers.Dense(16, activation = 'relu'),\n",
    "                      keras.layers.Dense(7, activation = 'softmax'),\n",
    "  ])\n",
    " \n",
    "    return model\n",
    "def build_model_VGG16_binary():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 2 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input layer\n",
    "    input_rgb = layers.Input(shape=(224, 224, 3), name='input_rgb')\n",
    "    \n",
    "    # Create the VGG16 model without the top classification layers\n",
    "    base_model = VGG16(include_top=False, weights='imagenet', input_tensor=input_rgb)\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    base_model.trainable = True\n",
    "\n",
    "    # Get the output of the base model\n",
    "    x = base_model(input_rgb)\n",
    "    \n",
    "    # Add additional layers to the base model output\n",
    "    x = keras.layers.AveragePooling2D(pool_size = (2 ,2))(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(64, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    x = keras.layers.Dense(16, activation='relu')(x)\n",
    "    \n",
    "    input_metadata = layers.Input(shape=(caract_metadata,), name='input_metadata')\n",
    "    x_final = keras.layers.Concatenate()([x, input_metadata])\n",
    "    \n",
    "    outputs = keras.layers.Dense(2, activation='softmax')(x_final)\n",
    "    \n",
    "    # Create the model\n",
    "    model = models.Model(inputs=[input_rgb, input_metadata], outputs=outputs)\n",
    "     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_VGG16_flag == True:\n",
    "    # Mostrar estructura de la red\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_VGG16()\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_VGG16_binary()\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_VGG16_flag == True:\n",
    "    # Hiperparámetros\n",
    "    # epochs\n",
    "    n_epochs = [50]\n",
    "    # batch sizes\n",
    "    batch_sizes = [8, 16, 32,64]\n",
    "    # learning rates\n",
    "    learning_rates = [0.001, 0.0005, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_VGG16_flag == True:\n",
    "    # Diccionario para almacenar los resultados del entrenamiento\n",
    "    history7 = {'epochs':[], 'batch_size':[], 'learning_rate':[],'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[], 'elapsed_time':[]}\n",
    "\n",
    "    # Variable per guardar el mejor accuracy obtenido con el conjunto de validación\n",
    "    best_val_acc = 0\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = def_callbacks()\n",
    "\n",
    "    # Iterar por todas las combinaciones de hiperparámetros\n",
    "    print('Entrenando el modelo VGG16...')\n",
    "    for epochs in n_epochs:\n",
    "        for batch_size in batch_sizes:\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "            valid_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "            \n",
    "            train_dataset = train_dataset.batch(batch_size)\n",
    "            valid_dataset = valid_dataset.batch(batch_size)\n",
    "            for learning_rate in learning_rates:\n",
    "                # Diccionario para guardar los resultados de un entrenamiento\n",
    "                histfit = {'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[]}\n",
    "                print('\\tHiperparámetros: epochs: {} | batch_size: {} | learning_rate: {}'.format(epochs, batch_size, learning_rate))\n",
    "                # Creación y compilación del modelo\n",
    "                if CF1_flag or CF4_flag:\n",
    "                    model = build_model_VGG16()\n",
    "                if CF2_flag or CF3_flag:\n",
    "                    model = build_model_VGG16_binary()\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "                if CF1_flag or CF4_flag:\n",
    "                    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "                if CF2_flag or CF3_flag:\n",
    "                    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])        \n",
    "                steps_per_epoch = np.ceil(len(x_train) / batch_size)\n",
    "                validation_steps = np.ceil(len(x_test) / batch_size)\n",
    "                validation_batch_size = batch_size\n",
    "                # Con Data Augmentation\n",
    "                validation_split = 0.0\n",
    "                # Cómputo del tiempo transcurrido durante el entrenamiento y validación\n",
    "                elapsed_time = timeit.timeit('fn_train_val2(histfit, model, x_train, x_train_ext, y_train, validation_data, batch_size, epochs,'+\n",
    "                                             'callbacks)', globals = globals(), number = 1)         \n",
    "                # Adición de los resultados al diccionario history7\n",
    "                history7['epochs'].append(epochs)\n",
    "                history7['batch_size'].append(batch_size)\n",
    "                history7['learning_rate'].append(learning_rate)\n",
    "                # Obtenemos el índice con el accuracy máximo con el conjunto de validación \n",
    "                max_val_acc_idx = histfit['val_accuracy'].index(max(histfit['val_accuracy']))\n",
    "                history7['loss'].append(histfit['loss'][max_val_acc_idx])\n",
    "                history7['accuracy'].append(histfit['accuracy'][max_val_acc_idx])\n",
    "                history7['val_loss'].append(histfit['val_loss'][max_val_acc_idx])\n",
    "                history7['val_accuracy'].append(histfit['val_accuracy'][max_val_acc_idx])\n",
    "                history7['elapsed_time'].append(elapsed_time)\n",
    "\n",
    "                # Guardamos el modelo con mejor accuracy en la validación\n",
    "                if histfit['val_accuracy'][max_val_acc_idx] > best_val_acc:\n",
    "                    best_val_acc = histfit['val_accuracy'][max_val_acc_idx]\n",
    "                    df_histfit = pd.DataFrame.from_dict(histfit)\n",
    "                    df_histfit.to_pickle(output_path + 'df_histfit7')\n",
    "                    model.save(output_path + 'VGG16_best_model.h5')\n",
    "\n",
    "    # Conversión del dataframe a diccionario\n",
    "    df_history7 = pd.DataFrame.from_dict(history7)\n",
    "\n",
    "    # Guardado del dataframe con los resultados de todos los entrenamientos\n",
    "    df_history7.to_pickle(output_path + 'df_history7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_VGG16_flag == True:\n",
    "    # Recuperación del dataframe con los resultados de los entrenamientos con todas las combinaciones de hiperparámetros definidas\n",
    "    df_history7 = pd.read_pickle(output_path + 'df_history7')\n",
    "    \n",
    "    # Obtención de las 5 combinaciones de hiperparámetros que han devuelto mejor accuracy en la validación  \n",
    "    df_history7.sort_values(by = 'val_accuracy', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_VGG16_flag == True:\n",
    "    # Recuperación del dataframe con los resultados del entramiento con los hiperparámetros que han devuelto mejor resultados en la validación\n",
    "    df_histfit7 = pd.read_pickle(output_path + 'df_histfit7')\n",
    "    \n",
    "    # Representación de las gráficas con el accuracy y loss del mejor modelo obtenido\n",
    "    plot_acc_loss(df_histfit7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model_VGG16_flag == True:\n",
    "    # Evaluación del modelo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_VGG16()\n",
    "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_VGG16_binary()\n",
    "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.load_weights(output_path + 'VGG16_best_model.h5')\n",
    "    #model.save(output_path + 'model7.h5') # guardado del modelo completo\n",
    "\n",
    "    loss, acc = model.evaluate([x_test,x_test_ext], y_test, verbose = 2) # Evaluación con el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_VGG16_flag == True:\n",
    "    # Carga del modelo completo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_VGG16()\n",
    "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_VGG16_binary()\n",
    "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.load_weights(output_path + 'VGG16_best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_VGG16_flag == True:\n",
    "    # Predicción con el conjunto de test\n",
    "    y_pred = model.predict([x_test,x_test_ext])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_VGG16_flag == True:\n",
    "    # Curva ROC multiclase\n",
    "    plot_multiclass_roc(model, [x_test,x_test_ext], y_test, classes, len(classes), figsize = (14, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_VGG16_flag == True:\n",
    "    # Matriz de confusión\n",
    "    cm = confusion_matrix(y_test, y_pred.argmax(axis = 1))\n",
    "    \n",
    "    # Cálculo de la matriz de confusión y representación normalizada y sin normalizar\n",
    "    plot_confusion_matrix(cm, cm_plot_classes, normalize = True)\n",
    "    plot_confusion_matrix(cm, cm_plot_classes, normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_VGG16_flag == True:\n",
    "    # Métricas\n",
    "    print('***** MÉTRICAS *****')\n",
    "    print('Accuracy: {:.5f}'.format(metrics.accuracy_score(y_test, y_pred.argmax(axis = 1))))\n",
    "    print('Precision: {:.5f}'.format(metrics.precision_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "    print('Recall: {:.5f}'.format(metrics.recall_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "    print('F1 Score: {:.5f}'.format(metrics.f1_score(y_test, y_pred.argmax(axis = 1), average ='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_VGG16_flag == True:\n",
    "    # Dataframe temporal para guardar los accuracies de cada clase de este modelo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        m_temp = pd.DataFrame(columns = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6'], dtype = np.float64) \n",
    "    if CF2_flag or CF3_flag:\n",
    "        m_temp = pd.DataFrame(columns = ['C0', 'C1'], dtype = np.float64)\n",
    "    for i in range(len(classes)):\n",
    "        m_temp['C'+str(i)] = [round(cm[i][i]/np.sum(cm[i]),5)]\n",
    "        \n",
    "    # Adición de la fila con el accuracy de cada clase de este modelo a la matriz con el resto de modelos\n",
    "    m1 = pd.concat([m1, m_temp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Model-8-%C2%B7-DenseNet201\">Model 8 · DenseNet201<a class=\"anchor-link\" href=\"#Model-8-%C2%B7-DenseNet201\">¶</a></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_DenseNet201():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 7 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = keras.layers.Input(shape = (224, 224, 3))\n",
    "    model = DenseNet201(include_top = False,                   # no incluir la capa de clasificación\n",
    "                        input_tensor = inputs,\n",
    "                        weights = 'imagenet')                  # preentrenado con el dataset imagenet.\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    model.trainable = True\n",
    "    \n",
    "    model = Sequential([model, \n",
    "                      keras.layers.AveragePooling2D(pool_size = (2 ,2)),      \n",
    "                      keras.layers.Flatten(), \n",
    "                      keras.layers.Dense(64, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.2), \n",
    "                      keras.layers.Dense(16, activation = 'relu'),\n",
    "                      keras.layers.Dense(7, activation = 'softmax'),\n",
    "  ])\n",
    " \n",
    "    return model\n",
    "def build_model_DenseNet201_binary():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 2 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input layer\n",
    "    input_rgb = layers.Input(shape=(224, 224, 3), name='input_rgb')\n",
    "    \n",
    "    # Create the DenseNet201 model without the top classification layers\n",
    "    base_model = DenseNet201(include_top=False, weights='imagenet', input_tensor=input_rgb)\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    base_model.trainable = True\n",
    "\n",
    "    # Get the output of the base model\n",
    "    x = base_model(input_rgb)\n",
    "    \n",
    "    # Add additional layers to the base model output\n",
    "    x = keras.layers.AveragePooling2D(pool_size = (2 ,2))(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(64, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    x = keras.layers.Dense(16, activation='relu')(x)\n",
    "    \n",
    "    input_metadata = layers.Input(shape=(caract_metadata,), name='input_metadata')\n",
    "    x_final = keras.layers.Concatenate()([x, input_metadata])\n",
    "    \n",
    "    outputs = keras.layers.Dense(2, activation='softmax')(x_final)\n",
    "    \n",
    "    # Create the model\n",
    "    model = models.Model(inputs=[input_rgb, input_metadata], outputs=outputs)\n",
    " \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_densenet_flag == True:\n",
    "    # Mostrar estructura de la red\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_DenseNet201()\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_DenseNet201_binary()\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_densenet_flag == True:\n",
    "    # Hiperparámetros\n",
    "    # epochs\n",
    "    n_epochs = [50]\n",
    "    # batch sizes\n",
    "    batch_sizes = [8, 12, 16]\n",
    "    # learning rates\n",
    "    learning_rates = [0.001, 0.0005, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_densenet_flag == True:\n",
    "    # Diccionario para almacenar los resultados del entrenamiento\n",
    "    history8 = {'epochs':[], 'batch_size':[], 'learning_rate':[],'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[], 'elapsed_time':[]}\n",
    "\n",
    "    # Variable per guardar el mejor accuracy obtenido con el conjunto de validación\n",
    "    best_val_acc = 0\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = def_callbacks()\n",
    "\n",
    "    # Iterar por todas las combinaciones de hiperparámetros\n",
    "    print('Entrenando el modelo DenseNet201...')\n",
    "    for epochs in n_epochs:\n",
    "        for batch_size in batch_sizes:\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "            valid_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "            \n",
    "            train_dataset = train_dataset.batch(batch_size)\n",
    "            valid_dataset = valid_dataset.batch(batch_size)\n",
    "            for learning_rate in learning_rates:\n",
    "                # Diccionario para guardar los resultados de un entrenamiento\n",
    "                histfit = {'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[]}\n",
    "                print('\\tHiperparámetros: epochs: {} | batch_size: {} | learning_rate: {}'.format(epochs, batch_size, learning_rate))\n",
    "                # Creación y compilación del modelo\n",
    "                if CF1_flag or CF4_flag:\n",
    "                    model = build_model_DenseNet201()\n",
    "                if CF2_flag or CF3_flag:\n",
    "                    model = build_model_DenseNet201_binary()\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "                if CF1_flag or CF4_flag:\n",
    "                    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "                if CF2_flag or CF3_flag:\n",
    "                    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])        \n",
    "                steps_per_epoch = np.ceil(len(x_train) / batch_size)\n",
    "                validation_steps = np.ceil(len(x_test) / batch_size)\n",
    "                validation_batch_size = batch_size\n",
    "                # Con Data Augmentation\n",
    "                validation_split = 0.0\n",
    "                # Cómputo del tiempo transcurrido durante el entrenamiento y validación\n",
    "                elapsed_time = timeit.timeit('fn_train_val2(histfit, model, x_train, x_train_ext, y_train, validation_data, batch_size, epochs,'+\n",
    "                                             'callbacks)', globals = globals(), number = 1)         \n",
    "                # Adición de los resultados al diccionario history8\n",
    "                history8['epochs'].append(epochs)\n",
    "                history8['batch_size'].append(batch_size)\n",
    "                history8['learning_rate'].append(learning_rate)\n",
    "                # Obtenemos el índice con el accuracy máximo con el conjunto de validación \n",
    "                max_val_acc_idx = histfit['val_accuracy'].index(max(histfit['val_accuracy']))\n",
    "                history8['loss'].append(histfit['loss'][max_val_acc_idx])\n",
    "                history8['accuracy'].append(histfit['accuracy'][max_val_acc_idx])\n",
    "                history8['val_loss'].append(histfit['val_loss'][max_val_acc_idx])\n",
    "                history8['val_accuracy'].append(histfit['val_accuracy'][max_val_acc_idx])\n",
    "                history8['elapsed_time'].append(elapsed_time)\n",
    "\n",
    "                # Guardamos el modelo con mejor accuracy en la validación\n",
    "                if histfit['val_accuracy'][max_val_acc_idx] > best_val_acc:\n",
    "                    best_val_acc = histfit['val_accuracy'][max_val_acc_idx]\n",
    "                    df_histfit = pd.DataFrame.from_dict(histfit)\n",
    "                    df_histfit.to_pickle(output_path + 'df_histfit8')\n",
    "                    model.save(output_path + 'DenseNet201_best_model.h5')\n",
    "\n",
    "    # Conversión del dataframe a diccionario\n",
    "    df_history8 = pd.DataFrame.from_dict(history8)\n",
    "\n",
    "    # Guardado del dataframe con los resultados de todos los entrenamientos\n",
    "    df_history8.to_pickle(output_path + 'df_history8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_densenet_flag == True:\n",
    "    # Recuperación del dataframe con los resultados de los entrenamientos con todas las combinaciones de hiperparámetros definidas\n",
    "    df_history8 = pd.read_pickle(output_path + 'df_history8')\n",
    "    \n",
    "    # Obtención de las 5 combinaciones de hiperparámetros que han devuelto mejor accuracy en la validación  \n",
    "    df_history8.sort_values(by = 'val_accuracy', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_densenet_flag == True:\n",
    "    # Recuperación del dataframe con los resultados del entramiento con los hiperparámetros que han devuelto mejor resultados en la validación\n",
    "    df_histfit8 = pd.read_pickle(output_path + 'df_histfit8')\n",
    "    \n",
    "    # Representación de las gráficas con el accuracy y loss del mejor modelo obtenido\n",
    "    plot_acc_loss(df_histfit8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model_densenet_flag == True:\n",
    "    # Evaluación del modelo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_DenseNet201()\n",
    "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_DenseNet201_binary()\n",
    "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.load_weights(output_path + 'DenseNet201_best_model.h5')\n",
    "    #model.save(output_path + 'model8.h5') # guardado del modelo completo\n",
    "    \n",
    "    loss, acc = model.evaluate([x_test,x_test_ext], y_test, verbose = 2) # Evaluación con el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_densenet_flag == True:\n",
    "    # Carga del modelo completo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_DenseNet201()\n",
    "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_DenseNet201_binary()\n",
    "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.load_weights(output_path + 'model8.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_densenet_flag == True:\n",
    "    # Predicción con el conjunto de test\n",
    "    y_pred = model.predict([x_test,x_test_ext], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_densenet_flag == True:\n",
    "    # Curva ROC multiclase\n",
    "    plot_multiclass_roc(model, [x_test,x_test_ext], y_test, classes, len(classes), figsize = (14, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_densenet_flag == True:\n",
    "    # Matriz de confusión\n",
    "    cm = confusion_matrix(y_test, y_pred.argmax(axis = 1))\n",
    "    \n",
    "    # Cálculo de la matriz de confusión y representación normalizada y sin normalizar\n",
    "    plot_confusion_matrix(cm, cm_plot_classes, normalize = True)\n",
    "    plot_confusion_matrix(cm, cm_plot_classes, normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_densenet_flag == True:\n",
    "    # Métricas\n",
    "    print('***** MÉTRICAS *****')\n",
    "    print('Accuracy: {:.5f}'.format(metrics.accuracy_score(y_test, y_pred.argmax(axis = 1))))\n",
    "    print('Precision: {:.5f}'.format(metrics.precision_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "    print('Recall: {:.5f}'.format(metrics.recall_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "    print('F1 Score: {:.5f}'.format(metrics.f1_score(y_test, y_pred.argmax(axis = 1), average ='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_densenet_flag == True:\n",
    "    # Dataframe temporal para guardar los accuracies de cada clase de este modelo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        m_temp = pd.DataFrame(columns = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6'], dtype = np.float64) \n",
    "    if CF2_flag or CF3_flag:\n",
    "        m_temp = pd.DataFrame(columns = ['C0', 'C1'], dtype = np.float64)\n",
    "    for i in range(len(classes)):\n",
    "        m_temp['C'+str(i)] = [round(cm[i][i]/np.sum(cm[i]),5)]\n",
    "        \n",
    "    # Adición de la fila con el accuracy de cada clase de este modelo a la matriz con el resto de modelos\n",
    "    m1 = pd.concat([m1, m_temp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Model-9-%C2%B7-MobileNetV2\">Model 9 · MobileNetV2<a class=\"anchor-link\" href=\"#Model-9-%C2%B7-MobileNetV2\">¶</a></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_MobileNetV2():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 7 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = keras.layers.Input(shape = (224, 224, 3))\n",
    "    model = MobileNetV2(include_top = False,                   # no incluir la capa de clasificación\n",
    "                      input_tensor = inputs,\n",
    "                      weights = 'imagenet')                  # preentrenado con el dataset imagenet.\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    model.trainable = True\n",
    "    \n",
    "    model = Sequential([model, \n",
    "                      keras.layers.AveragePooling2D(pool_size = (2 ,2)),      \n",
    "                      keras.layers.Flatten(), \n",
    "                      keras.layers.Dense(64, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.2), \n",
    "                      keras.layers.Dense(16, activation = 'relu'),\n",
    "                      keras.layers.Dense(7, activation = 'softmax'),\n",
    "  ])\n",
    " \n",
    "    return model\n",
    "def build_model_MobileNetV2_binary():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 2 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input layer\n",
    "    input_rgb = layers.Input(shape=(224, 224, 3), name='input_rgb')\n",
    "    \n",
    "    # Create the MobileNetV2 model without the top classification layers\n",
    "    base_model = MobileNetV2(include_top=False, weights='imagenet', input_tensor=input_rgb)\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    base_model.trainable = True\n",
    "\n",
    "    # Get the output of the base model\n",
    "    x = base_model(input_rgb)\n",
    "    \n",
    "    # Add additional layers to the base model output\n",
    "    x = keras.layers.AveragePooling2D(pool_size = (2 ,2))(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(64, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    x = keras.layers.Dense(16, activation='relu')(x)\n",
    "    \n",
    "    input_metadata = layers.Input(shape=(caract_metadata,), name='input_metadata')\n",
    "    x_final = keras.layers.Concatenate()([x, input_metadata])\n",
    "    \n",
    "    outputs = keras.layers.Dense(2, activation='softmax')(x_final)\n",
    "    \n",
    "    # Create the model\n",
    "    model = models.Model(inputs=[input_rgb, input_metadata], outputs=outputs)\n",
    " \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_mobilenet_flag == True:\n",
    "    # Mostrar estructura de la red\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_MobileNetV2()\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_MobileNetV2_binary()\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_mobilenet_flag == True:\n",
    "    # Hiperparámetros\n",
    "    # epochs\n",
    "    n_epochs = [50]\n",
    "    # batch sizes\n",
    "    batch_sizes = [8, 16, 32,64]\n",
    "    # learning rates\n",
    "    learning_rates = [0.001, 0.0005, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_mobilenet_flag == True:\n",
    "    # Diccionario para almacenar los resultados del entrenamiento\n",
    "    history9 = {'epochs':[], 'batch_size':[], 'learning_rate':[],'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[], 'elapsed_time':[]}\n",
    "\n",
    "    # Variable per guardar el mejor accuracy obtenido con el conjunto de validación\n",
    "    best_val_acc = 0\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = def_callbacks()\n",
    "\n",
    "    # Iterar por todas las combinaciones de hiperparámetros\n",
    "    print('Entrenando el modelo MobileNetV2...')\n",
    "    for epochs in n_epochs:\n",
    "        for batch_size in batch_sizes:\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "            valid_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "            \n",
    "            train_dataset = train_dataset.batch(batch_size)\n",
    "            valid_dataset = valid_dataset.batch(batch_size)\n",
    "            for learning_rate in learning_rates:\n",
    "                # Diccionario para guardar los resultados de un entrenamiento\n",
    "                histfit = {'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[]}\n",
    "                print('\\tHiperparámetros: epochs: {} | batch_size: {} | learning_rate: {}'.format(epochs, batch_size, learning_rate))\n",
    "                # Creación y compilación del modelo\n",
    "                if CF1_flag or CF4_flag:\n",
    "                    model = build_model_MobileNetV2()\n",
    "                if CF2_flag or CF3_flag:\n",
    "                    model = build_model_MobileNetV2_binary()\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "                if CF1_flag or CF4_flag:\n",
    "                    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "                if CF2_flag or CF3_flag:\n",
    "                    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])       \n",
    "                steps_per_epoch = np.ceil(len(x_train) / batch_size)\n",
    "                validation_steps = np.ceil(len(x_test) / batch_size)\n",
    "                validation_batch_size = batch_size\n",
    "                # Con Data Augmentation\n",
    "                validation_split = 0.0\n",
    "                # Cómputo del tiempo transcurrido durante el entrenamiento y validación\n",
    "                elapsed_time = timeit.timeit('fn_train_val2(histfit, model, x_train, x_train_ext, y_train, validation_data, batch_size, epochs,'+\n",
    "                                             'callbacks)', globals = globals(), number = 1)           \n",
    "                # Adición de los resultados al diccionario history9\n",
    "                history9['epochs'].append(epochs)\n",
    "                history9['batch_size'].append(batch_size)\n",
    "                history9['learning_rate'].append(learning_rate)\n",
    "                # Obtenemos el índice con el accuracy máximo con el conjunto de validación \n",
    "                max_val_acc_idx = histfit['val_accuracy'].index(max(histfit['val_accuracy']))\n",
    "                history9['loss'].append(histfit['loss'][max_val_acc_idx])\n",
    "                history9['accuracy'].append(histfit['accuracy'][max_val_acc_idx])\n",
    "                history9['val_loss'].append(histfit['val_loss'][max_val_acc_idx])\n",
    "                history9['val_accuracy'].append(histfit['val_accuracy'][max_val_acc_idx])\n",
    "                history9['elapsed_time'].append(elapsed_time)\n",
    "\n",
    "                # Guardamos el modelo con mejor accuracy en la validación\n",
    "                if histfit['val_accuracy'][max_val_acc_idx] > best_val_acc:\n",
    "                    best_val_acc = histfit['val_accuracy'][max_val_acc_idx]\n",
    "                    df_histfit = pd.DataFrame.from_dict(histfit)\n",
    "                    df_histfit.to_pickle(output_path + 'df_histfit9')\n",
    "                    model.save(output_path + 'MobileNetV2_best_model.h5')\n",
    "\n",
    "    # Conversión del dataframe a diccionario\n",
    "    df_history9 = pd.DataFrame.from_dict(history9)\n",
    "\n",
    "    # Guardado del dataframe con los resultados de todos los entrenamientos\n",
    "    df_history9.to_pickle(output_path + 'df_history9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_mobilenet_flag == True:\n",
    "    # Recuperación del dataframe con los resultados de los entrenamientos con todas las combinaciones de hiperparámetros definidas\n",
    "    df_history9 = pd.read_pickle(output_path + 'df_history9')\n",
    "    \n",
    "    # Obtención de las 5 combinaciones de hiperparámetros que han devuelto mejor accuracy en la validación  \n",
    "    df_history9.sort_values(by = 'val_accuracy', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_mobilenet_flag == True:  \n",
    "    # Recuperación del dataframe con los resultados del entramiento con los hiperparámetros que han devuelto mejor resultados en la validación\n",
    "    df_histfit9 = pd.read_pickle(output_path + 'df_histfit9')\n",
    "    \n",
    "    # Representación de las gráficas con el accuracy y loss del mejor modelo obtenido\n",
    "    plot_acc_loss(df_histfit9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model_mobilenet_flag == True:\n",
    "    # Evaluación del modelo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_MobileNetV2()\n",
    "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_MobileNetV2_binary()\n",
    "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.load_weights(output_path + 'MobileNetV2_best_model.h5')\n",
    "    #model.save(output_path + 'model9.h5') # guardado del modelo completo\n",
    "\n",
    "    loss, acc = model.evaluate([x_test,x_test_ext], y_test, verbose = 2) # Evaluación con el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_mobilenet_flag == True:\n",
    "    # Carga del modelo completo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_MobileNetV2()\n",
    "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_MobileNetV2_binary()\n",
    "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.load_weights(output_path + 'model9.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_mobilenet_flag == True:\n",
    "    # Predicción con el conjunto de test\n",
    "    y_pred = model.predict([x_test,x_test_ext])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_mobilenet_flag == True:\n",
    "    # Curva ROC multiclase\n",
    "    plot_multiclass_roc(model, [x_test,x_test_ext], y_test, classes, len(classes), figsize = (14, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_mobilenet_flag == True:  \n",
    "    # Matriz de confusión\n",
    "    cm = confusion_matrix(y_test, y_pred.argmax(axis = 1))\n",
    "    \n",
    "    # Cálculo de la matriz de confusión y representación normalizada y sin normalizar\n",
    "    plot_confusion_matrix(cm, cm_plot_classes, normalize = True)\n",
    "    plot_confusion_matrix(cm, cm_plot_classes, normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_mobilenet_flag == True:\n",
    "    # Métricas\n",
    "    print('***** MÉTRICAS *****')\n",
    "    print('Accuracy: {:.5f}'.format(metrics.accuracy_score(y_test, y_pred.argmax(axis = 1))))\n",
    "    print('Precision: {:.5f}'.format(metrics.precision_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "    print('Recall: {:.5f}'.format(metrics.recall_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "    print('F1 Score: {:.5f}'.format(metrics.f1_score(y_test, y_pred.argmax(axis = 1), average ='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_mobilenet_flag == True:\n",
    "    # Dataframe temporal para guardar los accuracies de cada clase de este modelo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        m_temp = pd.DataFrame(columns = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6'], dtype = np.float64) \n",
    "    if CF2_flag or CF3_flag:\n",
    "        m_temp = pd.DataFrame(columns = ['C0', 'C1'], dtype = np.float64)\n",
    "    for i in range(len(classes)):\n",
    "        m_temp['C'+str(i)] = [round(cm[i][i]/np.sum(cm[i]),5)]\n",
    "        \n",
    "    # Adición de la fila con el accuracy de cada clase de este modelo a la matriz con el resto de modelos\n",
    "    m1 = pd.concat([m1, m_temp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Model-10-%C2%B7-InceptionResNetV2\">Model 10 · InceptionResNetV2<a class=\"anchor-link\" href=\"#Model-10-%C2%B7-InceptionResNetV2\">¶</a></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_InceptionResNetV2():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 7 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = keras.layers.Input(shape = (224, 224, 3))\n",
    "    model = InceptionResNetV2(include_top = False,                   # no incluir la capa de clasificación\n",
    "                      input_tensor = inputs,\n",
    "                      weights = 'imagenet')                  # preentrenado con el dataset imagenet.\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    model.trainable = True\n",
    "    \n",
    "    model = Sequential([model, \n",
    "                      keras.layers.AveragePooling2D(pool_size = (2 ,2)),      \n",
    "                      keras.layers.Flatten(), \n",
    "                      keras.layers.Dense(64, activation = 'relu'),\n",
    "                      keras.layers.Dropout(0.2), \n",
    "                      keras.layers.Dense(16, activation = 'relu'),\n",
    "                      keras.layers.Dense(7, activation = 'softmax'),\n",
    "  ])\n",
    " \n",
    "    return model\n",
    "def build_model_InceptionResNetV2_binary():\n",
    "    \n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal que utiliza la red EfficientNet B0, preentrenada con el dataset imagenet,\n",
    "    al que se le sustituyen las capas top de clasificación por las siguientes:\n",
    "    1 capa de pooling de media global 2D,\n",
    "    1 capa de normalización del batch,\n",
    "    1 capa del 20% de descartes,\n",
    "    1 capa densa de salida con 2 neuronas y función de activación Softmax.\n",
    "    \n",
    "    Devuelve:\n",
    "    model -- objecto de Keras que representa la red neuronal convolucional\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input layer\n",
    "    input_rgb = layers.Input(shape=(224, 224, 3), name='input_rgb')\n",
    "    \n",
    "    # Create the InceptionResNetV2 model without the top classification layers\n",
    "    base_model = InceptionResNetV2(include_top=False, weights='imagenet', input_tensor=input_rgb)\n",
    "\n",
    "    # Entrenar todas las capas, ajuste fino\n",
    "    base_model.trainable = True\n",
    "\n",
    "    # Get the output of the base model\n",
    "    x = base_model(input_rgb)\n",
    "    \n",
    "    # Add additional layers to the base model output\n",
    "    x = keras.layers.AveragePooling2D(pool_size = (2 ,2))(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(64, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    x = keras.layers.Dense(16, activation='relu')(x)\n",
    "    \n",
    "    input_metadata = layers.Input(shape=(caract_metadata,), name='input_metadata')\n",
    "    x_final = keras.layers.Concatenate()([x, input_metadata])\n",
    "    \n",
    "    outputs = keras.layers.Dense(2, activation='softmax')(x_final)\n",
    "    \n",
    "    # Create the model\n",
    "    model = models.Model(inputs=[input_rgb, input_metadata], outputs=outputs)\n",
    " \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_InceptionResNetV2_flag == True:\n",
    "    # Mostrar estructura de la red\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_InceptionResNetV2()\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_InceptionResNetV2_binary()\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_InceptionResNetV2_flag == True:\n",
    "    # Hiperparámetros\n",
    "    # epochs\n",
    "    n_epochs = [50]\n",
    "    # batch sizes\n",
    "    batch_sizes = [8, 16, 32,64]\n",
    "    # learning rates\n",
    "    learning_rates = [0.001, 0.0005, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_InceptionResNetV2_flag == True:\n",
    "    # Diccionario para almacenar los resultados del entrenamiento\n",
    "    history10 = {'epochs':[], 'batch_size':[], 'learning_rate':[],'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[], 'elapsed_time':[]}\n",
    "\n",
    "    # Variable per guardar el mejor accuracy obtenido con el conjunto de validación\n",
    "    best_val_acc = 0\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = def_callbacks()\n",
    "\n",
    "    # Iterar por todas las combinaciones de hiperparámetros\n",
    "    print('Entrenando el modelo InceptionResNetV2...')\n",
    "    for epochs in n_epochs:\n",
    "        for batch_size in batch_sizes:\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "            valid_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "            \n",
    "            train_dataset = train_dataset.batch(batch_size)\n",
    "            valid_dataset = valid_dataset.batch(batch_size)\n",
    "            for learning_rate in learning_rates:\n",
    "                # Diccionario para guardar los resultados de un entrenamiento\n",
    "                histfit = {'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[]}\n",
    "                print('\\tHiperparámetros: epochs: {} | batch_size: {} | learning_rate: {}'.format(epochs, batch_size, learning_rate))\n",
    "                # Creación y compilación del modelo\n",
    "                if CF1_flag or CF4_flag:\n",
    "                    model = build_model_InceptionResNetV2()\n",
    "                if CF2_flag or CF3_flag:\n",
    "                    model = build_model_InceptionResNetV2_binary()\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "                if CF1_flag or CF4_flag:\n",
    "                    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "                if CF2_flag or CF3_flag:\n",
    "                    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])       \n",
    "                steps_per_epoch = np.ceil(len(x_train) / batch_size)\n",
    "                validation_steps = np.ceil(len(x_test) / batch_size)\n",
    "                validation_batch_size = batch_size\n",
    "                # Con Data Augmentation\n",
    "                validation_split = 0.0\n",
    "                # Cómputo del tiempo transcurrido durante el entrenamiento y validación\n",
    "                elapsed_time = timeit.timeit('fn_train_val2(histfit, model, x_train, x_train_ext, y_train, validation_data, batch_size, epochs,'+\n",
    "                                             'callbacks)', globals = globals(), number = 1)            \n",
    "                # Adición de los resultados al diccionario history10\n",
    "                history10['epochs'].append(epochs)\n",
    "                history10['batch_size'].append(batch_size)\n",
    "                history10['learning_rate'].append(learning_rate)\n",
    "                # Obtenemos el índice con el accuracy máximo con el conjunto de validación \n",
    "                max_val_acc_idx = histfit['val_accuracy'].index(max(histfit['val_accuracy']))\n",
    "                history10['loss'].append(histfit['loss'][max_val_acc_idx])\n",
    "                history10['accuracy'].append(histfit['accuracy'][max_val_acc_idx])\n",
    "                history10['val_loss'].append(histfit['val_loss'][max_val_acc_idx])\n",
    "                history10['val_accuracy'].append(histfit['val_accuracy'][max_val_acc_idx])\n",
    "                history10['elapsed_time'].append(elapsed_time)\n",
    "\n",
    "                # Guardamos el modelo con mejor accuracy en la validación\n",
    "                if histfit['val_accuracy'][max_val_acc_idx] > best_val_acc:\n",
    "                    best_val_acc = histfit['val_accuracy'][max_val_acc_idx]\n",
    "                    df_histfit = pd.DataFrame.from_dict(histfit)\n",
    "                    df_histfit.to_pickle(output_path + 'df_histfit10')\n",
    "                    model.save(output_path + 'InceptionResNetV2_best_model.h5')\n",
    "\n",
    "    # Conversión del dataframe a diccionario\n",
    "    df_history10 = pd.DataFrame.from_dict(history10)\n",
    "\n",
    "    # Guardado del dataframe con los resultados de todos los entrenamientos\n",
    "    df_history10.to_pickle(output_path + 'df_history10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_InceptionResNetV2_flag == True:\n",
    "    # Recuperación del dataframe con los resultados de los entrenamientos con todas las combinaciones de hiperparámetros definidas\n",
    "    df_history10 = pd.read_pickle(output_path + 'df_history10')\n",
    "    \n",
    "    # Obtención de las 5 combinaciones de hiperparámetros que han devuelto mejor accuracy en la validación  \n",
    "    df_history10.sort_values(by = 'val_accuracy', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_InceptionResNetV2_flag == True:\n",
    "    # Recuperación del dataframe con los resultados del entramiento con los hiperparámetros que han devuelto mejor resultados en la validación\n",
    "    df_histfit10 = pd.read_pickle(output_path + 'df_histfit10')\n",
    "    \n",
    "    # Representación de las gráficas con el accuracy y loss del mejor modelo obtenido\n",
    "    plot_acc_loss(df_histfit10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model_InceptionResNetV2_flag == True:\n",
    "    # Evaluación del modelo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_InceptionResNetV2()\n",
    "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_InceptionResNetV2_binary()\n",
    "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.load_weights(output_path + 'InceptionResNetV2_best_model.h5')\n",
    "    model.save(output_path + 'model10.h5') # guardado del modelo completo\n",
    "\n",
    "    loss, acc = model.evaluate([x_test,x_test_ext], y_test, verbose = 2) # Evaluación con el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_InceptionResNetV2_flag == True:\n",
    "    # Carga del modelo completo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        model = build_model_InceptionResNetV2()\n",
    "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    if CF2_flag or CF3_flag:\n",
    "        model = build_model_InceptionResNetV2_binary()\n",
    "        model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.load_weights(output_path + 'model10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_InceptionResNetV2_flag == True:\n",
    "    # Predicción con el conjunto de test\n",
    "    y_pred = model.predict([x_test,x_test_ext])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_InceptionResNetV2_flag == True:\n",
    "    # Curva ROC multiclase\n",
    "    plot_multiclass_roc(model, [x_test,x_test_ext], y_test, classes, len(classes), figsize = (14, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_InceptionResNetV2_flag == True:\n",
    "    # Matriz de confusión\n",
    "    cm = confusion_matrix(y_test, y_pred.argmax(axis = 1))\n",
    "    \n",
    "    # Cálculo de la matriz de confusión y representación normalizada y sin normalizar\n",
    "    plot_confusion_matrix(cm, cm_plot_classes, normalize = True)\n",
    "    plot_confusion_matrix(cm, cm_plot_classes, normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_InceptionResNetV2_flag == True:\n",
    "    # Métricas\n",
    "    print('***** MÉTRICAS *****')\n",
    "    print('Accuracy: {:.5f}'.format(metrics.accuracy_score(y_test, y_pred.argmax(axis = 1))))\n",
    "    print('Precision: {:.5f}'.format(metrics.precision_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "    print('Recall: {:.5f}'.format(metrics.recall_score(y_test, y_pred.argmax(axis = 1), average ='macro')))\n",
    "    print('F1 Score: {:.5f}'.format(metrics.f1_score(y_test, y_pred.argmax(axis = 1), average ='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_InceptionResNetV2_flag == True:\n",
    "    # Dataframe temporal para guardar los accuracies de cada clase de este modelo\n",
    "    if CF1_flag or CF4_flag:\n",
    "        m_temp = pd.DataFrame(columns = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6'], dtype = np.float64) \n",
    "    if CF2_flag or CF3_flag:\n",
    "        m_temp = pd.DataFrame(columns = ['C0', 'C1'], dtype = np.float64)\n",
    "    for i in range(len(classes)):\n",
    "        m_temp['C'+str(i)] = [round(cm[i][i]/np.sum(cm[i]),5)]\n",
    "        \n",
    "    # Adición de la fila con el accuracy de cada clase de este modelo a la matriz con el resto de modelos\n",
    "    m1 = pd.concat([m1, m_temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinicio de los índices\n",
    "m1 = m1.reset_index()\n",
    "# Borrado de la columna index\n",
    "m1 = m1.drop(['index'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_matriz_pesos_flag == True:\n",
    "    # Guardado de la matriz de pesos de los modelos\n",
    "    m1.to_pickle(output_path + 'm1')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4958955,
     "sourceId": 8440391,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5039767,
     "sourceId": 8455995,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30588,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
